# 开发指南

## 任务设计

对于一个工具而言，它能发生的所有变化就是新增、移除和更新。问题是，如何模拟这些变化（尤其是更新，如果只是更新工具的描述，LLM 往往不会受影响），以及如何设计合适的任务来针对性地考察模型的持续学习能力

- **初始化**：给定初始的 200 个工具，作为已掌握的知识
    
    有一个小问题，是随机选出 200 个，还是每次都给一样的？随机选、多次取平均显然更公平，但是否过于消耗算力？如果过于消耗算力、但又想保持随机，是否可以减少每次新增工具的个数，这样就相当于增加了随机循环的轮次，但总开销保持不变
    
- **新增**：新增 100 个工具，模拟新技能的学习
    
    可能的评测指标：
    
    - **Learning Rate**：新知识的学习速度
    - **Retention Rate**：旧知识的保留率
- **移除**：移除 50 个工具
    
    考察模型如何应对已失效的知识。可以采用模拟报错的方式，或者直接维护一个可用的工具列表（但这样对 LLM 来说应该太简单了。我认为模拟报错更符合日常使用情况，LLM 面对报错需要弃用原有的工具，尝试采用其他方法实现）
    
    可能的评测指标：
    
    - **Error Recovery Rate**：修正次数 / 调用失效工具的次数（修正次数指的是模型在一次调用失效工具后，自我调整并最终完成任务的次数）。检查模型能否从调用失效工具中自我修正
    - **Obsolete Tool Recall**：调用失效工具次数 / 总调用次数。检查模型是否屡教不改
    - **Adaptation Delay**：从发现工具失效到成功调用新工具的平均时间或响应次数
    - **Selective Forgetting Score**：旧工具弃用后，agent 是否主动更新记忆（比较 task $N$ 和 task $N + 1$ 的调用分布，或比较第 $t$ 和 $t+1$ 轮循环同一个任务的调用分布）
    
    问题来了，如何保证移除了部分工具后，这个任务还可以实现？如果任务已经不能实现了，那 LLM 要么一直试图调用失效的工具，要么产生一个幻觉的结果，但这样会导致我们设计的指标出现问题
    
    对于复杂的问题，很难给出一个唯一的工具依赖。因此，我提议放弃任务的多工具属性，为每个工具设计一个任务。但这样工具移除后模型就无法完成任务了，这一条评测方式也就失效了
    
- **更新**：更新 50 个工具，更新参数接口 / 将工具迁移到新 server
    - **描述**：改变工具的使用说明和示例。更改描述很可能对 LLM 不起作用，因为语义没有发生变化，而 LLM 对语法的变化并不敏感
    - **接口**：修改参数、参数名、必选字段、默认值等
    - **返回结果**：修改返回格式或含义（如何实现？）、添加随机延时？

**其他需求**：Agent 的学习和遗忘曲线的可视化

## 一些实现细节

- **如何判定任务是否实现**：参考 LiveMCPBench，每个任务配有 key points 作为必须子步骤。并由 LLM 根据任务描述、key points 判断 agent 是否完成任务（论文中由人工标注验证了 LLM 的一致性）
    
    这样的实现存在一个问题，就是 LLM 的准确度依旧堪忧（Which 在 LiveMCPBench 中也有提到）。是否存在一种方法，让调用的结果有唯一正确解（或者至少存在某种直接的评判方式，而非通过模棱两可的 LLM 来判断），并且保留任务多步、多工具的属性？
    
    其次，任务的个数也不够，不满足多轮循环的要求。而且，如果工具的顺序是随机的，如果不调整相应的任务，可能会出现分配的任务无法实现的情况。因此，是否需要构建一个任务-工具依赖，用图算法来保证依赖始终满足？还是说可以放弃工具顺序的随机性，将它们固定下来？或者放弃任务的多工具属性，为每个工具设计一个相应的任务？（但这样似乎有些太简单了。优点是，依赖唯一确定，非常容易实现，可以让 LLM 将每个工具的一个示例转化为自然语言，这样正确答案就是那个示例）

## 开发步骤

1. 将 LiveMCPBench 改为动态，首先支持工具的增加和删除
2. 添加对工具更新的支持（编写能够随机更改工具的接口的脚本，包括输入的参数名、必须参数、默认值等，以及输出的格式、含义等。注意不要对原工具进行破坏性操作）
3. 其余内容再议